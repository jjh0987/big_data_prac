{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e4e82cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tokenizers\n",
      "  Downloading tokenizers-0.11.6-cp38-cp38-macosx_11_0_arm64.whl (3.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.4 MB 813 kB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: tokenizers\n",
      "Successfully installed tokenizers-0.11.6\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/Users/junho/tensorflow_macos_venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87185f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [........................................................] 889842 / 889842"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ChatbotData.csv'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wget\n",
    "wget.download('https://github.com/songys/Chatbot_data/raw/master/ChatbotData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "422aa762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.rename('ChatbotData.csv','chatbot.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "642fc235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('chatbot.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b86526b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sample.txt','w',encoding='utf8') as f:\n",
    "    for row in df.itertuples():\n",
    "        f.write(row.Q+'\\n')\n",
    "        f.write(row.A+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e37ca2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import CharBPETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8523d3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe = CharBPETokenizer(lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f3047cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bpe.train(files='sample.txt',min_frequency=1,vocab_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cde5028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=7, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = bpe.encode('자연어 처리는 재밌다.')\n",
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb8c58e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4l/84htqxwn14n0jr_xvdc4tpbr0000gn/T/ipykernel_43673/1089915771.py:1: DeprecationWarning: Deprecated in 0.9.4: Encoding.words is deprecated, please use Encoding.word_ids instead.\n",
      "  enc.words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 1, 2, 2, 3]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3ff300f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2206, 1022, 797, 1875, 2251, 1028, 1535]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.ids # bpe token dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eecd50e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['자연', '어</w>', '처', '리는</w>', '재밌', '다</w>', '.</w>']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc1f641",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71eb82a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5664bec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('decomposition.txt','w',encoding='utf8') as f:\n",
    "    for row in df.itertuples():\n",
    "        q = unicodedata.normalize('NFD',row.Q)\n",
    "        f.write(q+'\\n')\n",
    "        a = unicodedata.normalize('NFD',row.A)\n",
    "        f.write(a+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56335576",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe = CharBPETokenizer(lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0fd4221b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bpe.train(files='decomposition.txt',min_frequency=1,vocab_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7b8e1720",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = unicodedata.normalize('NFD','자연어 처리는 재밌다.')\n",
    "enc = bpe.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a479f34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1290, 204, 299, 1547, 819, 1365, 158]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "94e6ad87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['자연', '어</w>', '처', '리는</w>', '재미', 'ᆻ다</w>', '.</w>']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe96ede",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6de11434",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "03fabef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "byte = ByteLevelBPETokenizer(lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e52f1b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "byte.train(files='sample.txt',min_frequency=1,vocab_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fee31ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = byte.encode('자연어 처리는 재밌다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "be8445ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2499, 272, 3487, 99, 1055, 2275, 293, 13]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b1c3c0a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ìŀĲìĹ°', 'ìĸ´', 'Ġì²ĺë', '¦', '¬ëĬĶ', 'Ġìŀ¬ë°Į', 'ëĭ¤', '.']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8114200e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7807f632",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b8c73283",
   "metadata": {},
   "outputs": [],
   "source": [
    "wp = BertWordPieceTokenizer(lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9b107383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wp.train(files='sample.txt',min_frequency=1,vocab_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c1a2f13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = wp.encode('자연어 처리는 재밌다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3f54e7b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1379, 201, 1014, 1028, 3349, 216, 10]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "79d6e167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['자연', '##어', '처', '##리는', '재밌', '##다', '.']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad797660",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c82e5f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import SentencePieceBPETokenizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
