{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fde6fd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle5 as pickle\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import date\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "from transformers import TextClassificationPipeline\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import TFBertForSequenceClassification\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime,timedelta\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76a71eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_client = MongoClient('mongodb://18.181.49.139:27017')\n",
    "mydb = my_client['final_project']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5550ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-06-09 16:48:05.492 ip-172-31-40-135:3534 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2022-06-09 16:48:05.513 ip-172-31-40-135:3534 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "# LSTM 토크나이저\n",
    "with open('data/lstm/goodtokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)  \n",
    "# LSTM 모델\n",
    "model = load_model('data/lstm/goodmodel.h5')\n",
    "# BERT 토크나이저, 모델\n",
    "loaded_tokenizer = BertTokenizerFast.from_pretrained('data/bert', from_pt=True)\n",
    "loaded_model = TFBertForSequenceClassification.from_pretrained('data/bert', from_pt=True)\n",
    "classifier = TextClassificationPipeline(tokenizer=loaded_tokenizer, model=loaded_model,\n",
    "                                            framework='tf', return_all_scores=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15f51b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_comment_csv():\n",
    "    try:\n",
    "        df_naver = pd.DataFrame(mydb['naver_feargreed'].find({'날짜':datetime.strftime((datetime.now()).date(), \"%Y-%m-%d\")})).drop('_id',axis=1)\n",
    "        df_kakao = pd.DataFrame(mydb['kakao_feargreed'].find({'날짜':datetime.strftime((datetime.now()).date(), \"%Y-%m-%d\")})).drop('_id',axis=1)\n",
    "    except:\n",
    "        df_naver = pd.DataFrame(mydb['naver_feargreed'].find({'날짜':datetime.strftime((datetime.now()).date()-timedelta(days=1), \"%Y-%m-%d\")})).drop('_id',axis=1)\n",
    "        df_kakao = pd.DataFrame(mydb['kakao_feargreed'].find({'날짜':datetime.strftime((datetime.now()).date()-timedelta(days=1), \"%Y-%m-%d\")})).drop('_id',axis=1)\n",
    "    \n",
    "    return df_naver, df_kakao\n",
    "    \n",
    "def get_code(symbol):\n",
    "#     krx = pd.read_csv('./src/krx_code.csv')\n",
    "#     krx = krx.set_index('한글 종목약명')\n",
    "#     try:\n",
    "#         code = krx.at[symbol,'단축코드']\n",
    "#         return code\n",
    "#     except:\n",
    "#         print('종목명을 다시 확인해주세요.')\n",
    "#         return 0\n",
    "    if symbol == '카카오':\n",
    "        code = '035720' # 카카오\n",
    "    else:\n",
    "        code = '035420' # NAVER\n",
    "    return code\n",
    "\n",
    "def get_comment(df,symbol):\n",
    "    code = get_code(symbol)\n",
    "    day = df['날짜'][0]\n",
    "    date_list = []\n",
    "    comment_list = []\n",
    "    raw_comment_list = []\n",
    "    chk = 1\n",
    "    i = 1\n",
    "    while chk:  \n",
    "        url = f'https://finance.naver.com/item/board.naver?code={code}&page={i}'\n",
    "        headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36 Edg/100.0.1185.50'}\n",
    "        res = requests.get(url, headers = headers)\n",
    "        bs = BeautifulSoup(res.text, 'html.parser')  \n",
    "        for j in range(20):\n",
    "            try:\n",
    "                root = bs.find('div',{'class':'section inner_sub'}).find_all('tr',{'onmouseover':'mouseOver(this)'})[j].text.split('\\n')                 \n",
    "                if day > root[1].split()[0].replace('.','-'):\n",
    "                    chk = 0\n",
    "                    break\n",
    "                if len(root) == 14: # 답글\n",
    "                    pass      \n",
    "                elif len(root) == 13: # 기본\n",
    "                    comment = root[3]\n",
    "                    date_list.append(root[1].split()[0].replace('.','-'))\n",
    "                    raw_comment_list.append(comment)            \n",
    "                else: # 에러\n",
    "                    pass\n",
    "            except: # 에러\n",
    "                pass\n",
    "            print(f'\\r{day} 댓글{len(raw_comment_list)}개 크롤링중..',end='')\n",
    "        i += 1\n",
    "        if chk == 0:\n",
    "            break   \n",
    "    print(f'\\r{day} 댓글{len(raw_comment_list)}개 크롤링완료')\n",
    "    df = pd.DataFrame()\n",
    "    df['날짜'] = date_list\n",
    "    df['댓글'] = raw_comment_list\n",
    "    return df\n",
    "\n",
    "def BERT_feargreed(df,symbol):\n",
    "    df = get_comment(df,symbol)  \n",
    "    raw_comment_list = df['댓글'].to_list()\n",
    "    pred_list=[]\n",
    "    for i in raw_comment_list:\n",
    "        a = classifier(i)[0]\n",
    "        f = a[0]['score']\n",
    "        g = a[1]['score']\n",
    "        if f >= g:\n",
    "            pred_list.append(1-f)\n",
    "        else:\n",
    "            pred_list.append(g)\n",
    "        print(f'\\rBERT 댓글{len(pred_list)}개 분석중..',end='')\n",
    "    df['BERT'] = pred_list\n",
    "    print('BERT분석 완료.')\n",
    "    return df\n",
    "\n",
    "def konlpy_okt(df,symbol):\n",
    "    df = BERT_feargreed(df,symbol)\n",
    "    okt = Okt()\n",
    "    tag_list = ['Noun','Verb','Adjective','VerbPrefix'] \n",
    "    comment_list = df['댓글'].to_list()\n",
    "    tokenized_data = []\n",
    "    for i in range(len(comment_list)):\n",
    "        tokenized_sentence = okt.pos(comment_list[i], stem=True) \n",
    "        tag_checked_sentence = []\n",
    "        for j in tokenized_sentence:\n",
    "            x,y = j\n",
    "            if y in tag_list:\n",
    "                tag_checked_sentence.append(x)\n",
    "        tokenized_data.append(tag_checked_sentence)     \n",
    "    for i in tokenized_data:\n",
    "        for j in range(len(i)):\n",
    "            i[j] = \"'\"+i[j]+\"'\"\n",
    "    df['LSTM'] = tokenized_data\n",
    "    return df\n",
    "    \n",
    "def feargreed_index(df,symbol):\n",
    "    df = konlpy_okt(df,symbol)\n",
    "    tokenized_data = df['LSTM'].to_list()\n",
    "    test = tokenizer.texts_to_sequences(tokenized_data)\n",
    "    test = pad_sequences(test, maxlen=15)\n",
    "    pred = model.predict(test)\n",
    "    df['LSTM'] = pred\n",
    "    df['LSTM'] = df['LSTM'].round(6)\n",
    "    print('LSTM분석 완료.')\n",
    "    return df\n",
    "\n",
    "def update_comment():\n",
    "    df_naver, df_kakao = read_comment_csv()\n",
    "    \n",
    "    print('NAVER 댓글 갱신중...')\n",
    "    df2_naver = feargreed_index(df_naver,'NAVER')\n",
    "    df_naver = df2_naver.append(df_naver).drop_duplicates(subset=['날짜','댓글'],keep='last')\n",
    "    df_naver = df_naver.reset_index(drop=True)\n",
    "    print('NAVER 댓글 갱신완료.')\n",
    "    \n",
    "    print('카카오 댓글 갱신중...')\n",
    "    df2_kakao = feargreed_index(df_kakao,'카카오')\n",
    "    df_kakao = df2_kakao.append(df_kakao).drop_duplicates(subset=['날짜','댓글'],keep='last')\n",
    "    df_kakao = df_kakao.reset_index(drop=True)\n",
    "    print('카카오 댓글 갱신완료.')\n",
    "    \n",
    "    #df_naver.to_csv('./streamlit/data/feargreed_naver.csv',index=False)\n",
    "    #df_kakao.to_csv('./streamlit/data/feargreed_kakao.csv',index=False)\n",
    "    mydb['naver_feargreed'].delete_many({'날짜':datetime.strftime((datetime.now()).date(), \"%Y-%m-%d\")})\n",
    "    mydb['kakao_feargreed'].delete_many({'날짜':datetime.strftime((datetime.now()).date(), \"%Y-%m-%d\")})\n",
    "\n",
    "    mydb['naver_feargreed'].insert_many(df_naver.to_dict('records'))\n",
    "    mydb['kakao_feargreed'].insert_many(df_kakao.to_dict('records'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57d5a9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAVER 댓글 갱신중...\n",
      "2022-06-08 댓글71개 크롤링완료.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6cf8199ba528>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mupdate_comment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-5b24442349cc>\u001b[0m in \u001b[0;36mupdate_comment\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NAVER 댓글 갱신중...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0mdf2_naver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeargreed_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_naver\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'NAVER'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m     \u001b[0mdf_naver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf2_naver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_naver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'날짜'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'댓글'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'last'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0mdf_naver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_naver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-5b24442349cc>\u001b[0m in \u001b[0;36mfeargreed_index\u001b[0;34m(df, symbol)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfeargreed_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkonlpy_okt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m     \u001b[0mtokenized_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'LSTM'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-5b24442349cc>\u001b[0m in \u001b[0;36mkonlpy_okt\u001b[0;34m(df, symbol)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mkonlpy_okt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBERT_feargreed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0mokt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOkt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mtag_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Noun'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Verb'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Adjective'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'VerbPrefix'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-5b24442349cc>\u001b[0m in \u001b[0;36mBERT_feargreed\u001b[0;34m(df, symbol)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mpred_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_comment_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'classifier' is not defined"
     ]
    }
   ],
   "source": [
    "update_comment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abd2a3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_client = MongoClient('mongodb://18.181.49.139:27017')\n",
    "mydb = my_client['final_project']\n",
    "kakao = pd.read_csv('data/feargreed_kakao_rev.csv')\n",
    "naver = pd.read_csv('data/feargreed_naver_rev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8662648a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x7fd56c489090>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydb['naver_feargreed'].insert_many(naver.to_dict('records'))\n",
    "mydb['kakao_feargreed'].insert_many(kakao.to_dict('records'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p37)",
   "language": "python",
   "name": "conda_tensorflow2_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
